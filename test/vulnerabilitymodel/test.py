# -*- coding: utf-8 -*-
"""
Created on Wed Feb  7 14:35:57 2024

@author: Moayad
"""

#%% Load dependencies

### import the vulnerability toolkit libraries
import sys
repoDir = 'C:/Users/Moayad/Documents/GitHub/stickModel'
sys.path.insert(1, f'{repoDir}')
from vulnerability import *
from utils import *

# Import other libraries
import pandas as pd
import numpy as np
import os
import time

#%% User Input

region = 'Region 1'
typology = 'S'
occupancy = 'Res'
outDir = f'{repoDir}/test/vulnerabilitymodel/output'

#%% Process SLFs

### Description of outputs

# slfs_to_export: DataFrame with SLFs sorted by component typology
# slfs:           Dictionary with SLFs sorted by component typology
# psd_slf:        Dictionary with summary of fitted SLFs to drift-sensitive components
# pfa_slf:        Dictionary with summary of fitted SLFs to acceleration-sensitive components
# psd_cache:      Dictionary with the intermediate outcome of slf generator for drift-sensitive components
# pfa_cache:      Dictionary with the intermediate outcome of slf generator for acceleration-sensitive components 

vulnModel = deriveVulnerability(region, typology, occupancy, repoDir)
slfs_to_export, slfs, psd_slf, pfa_slf, pfa_cache, psd_cache  = vulnModel.processSLFs(outDir)

#%% Plotting

rlz = 100 # Number of realisations to plot


### Initialise the figure
plt.figure(figsize=(12, 6))
ax1 = plt.subplot(1,2,1)
ax2 = plt.subplot(1,2,2)

for i in range(rlz):

    #######################
    ### Drift-Sensitive
    #######################

    ### get the psd lists
    a = list(psd_cache['1']['total_loss_storey'][i])
    maxval = np.max(psd_slf['1']['slf'])+np.max(pfa_slf['2']['slf'])
    ### get the normalized data
    y = [i/maxval for i in a]    
    ### plot individual realisations
    ax1.scatter(np.linspace(0,20,201), y, alpha = 0.7)

    ### plot 
    if i==1:    
        ### plot the normalised mean slf
        x = np.linspace(0,20,201)
        ax1.plot(x,psd_cache['1']['slfs']['mean']/maxval,linewidth = 6, color = 'black')
        ### plot the normalised median, 16th and 84th percentiles
        ax1.plot(x,psd_cache['1']['slfs'][0.5]/maxval, linewidth = 6, color = 'blue')
        ax1.plot(x,psd_cache['1']['slfs'][0.16]/maxval,linewidth = 6, color = 'green')
        ax1.plot(x,psd_cache['1']['slfs'][0.84]/maxval,linewidth = 6, color = 'red')
    
    ax1.set_xlabel(r'Peak Storey Drift, $\theta$ [%]')
    ax1.set_ylabel('Loss Ratio')
    ax1.grid(visible=True, which='major')
    ax1.grid(visible=True, which='minor')
    ax1.set_yticks(np.linspace(0,1,5))
    ax1.set_xlim([0, 10.0])
    ax1.set_ylim([0, 1.0])

    #######################
    ### Acceleration-Sensitive
    #######################
    
    ### get the pfa lists
    a = list(pfa_cache['2']['total_loss_storey'][i])
    maxval = np.max(psd_slf['1']['slf'])+np.max(pfa_slf['2']['slf'])
    ### get the normalized data
    y = [i/maxval for i in a]
    ### plot individual realisations
    ax2.scatter(np.linspace(0,5,201), y, alpha = 0.7)
    
    if i==1:
        
        ### plot the normalised mean
        x = np.linspace(0,5,201)
        ax2.plot(x,pfa_cache['2']['slfs']['mean']/maxval,linewidth = 6, color = 'black')
        ### plot the normalised median, 16th and 84th percentiles
        ax2.plot(x,pfa_cache['2']['slfs'][0.5]/maxval,linewidth = 6, color = 'blue')
        ax2.plot(x,pfa_cache['2']['slfs'][0.16]/maxval,linewidth = 6, color = 'green')
        ax2.plot(x,pfa_cache['2']['slfs'][0.84]/maxval,linewidth = 6, color = 'red')
    
    ax2.set_xlabel(r'Peak Floor Acceleration, $a_{max}$ [g]')
    ax2.set_ylabel('Loss Ratio')
    ax2.grid(visible=True, which='major')
    ax2.grid(visible=True, which='minor')
    ax2.set_yticks(np.linspace(0,1,5))
    ax2.set_xlim([0, 5.0])
    ax2.set_ylim([0, 1.0])

plt.show()


#%% Process Vulnerability Curves

path_to_analysis = f'{gitDir}/test/vulnerabilitymodel/analysis'
path_to_slfs = f'{gitDir}/metadata/{region}/output'

im = 'PGA'

im_list = ['PGA','SA(0.3s)','SA(0.6s)','SA(1.0s)','AvgSA(0.3s)','AvgSA(0.6s)','AvgSA(1.0s)'] # for batch analysis

analysis_files = os.listdir(path_to_analysis)
#analysis_files = ['CR_LFINF+CDL+DUH_H7']

for i in range(len(analysis_files)):
    
    # identify the typology
    typology = analysis_files[i]
    
    # select the appropriate slf
    if 'CR_' in typology:
        slfs = import_from_pkl(f'{path_to_slfs}/slf_cr_{occupancy}.pkl')
    elif 'MUR' in typology or 'MR_' in typology or 'MCF_' in typology and 'ADO' not in typology:
        slfs = import_from_pkl(f'{path_to_slfs}/slf_mcf_mr_{occupancy}.pkl')
    elif 'ADO' in typology:
        slfs = import_from_pkl(f'{path_to_slfs}/slf_ado_{occupancy}.pkl')
    elif 'W_' in typology:
        slfs = import_from_pkl(f'{path_to_slfs}/slf_w_{occupancy}.pkl')
    elif 'S_' in typology:
        slfs = import_from_pkl(f'{path_to_slfs}/slf_s_{occupancy}.pkl')
    else:
        slfs = import_from_pkl(f'{path_to_slfs}/slf_ado_{occupancy}.pkl')
                
    # get the intensity measure list
    if im == 'PGA':
        ims = pd.read_csv(f'{path_to_analysis}/{analysis_files[i]}/imls.csv', header= None)[0]
    elif im == 'SA(0.3s)':
        ims = pd.read_csv(f'{path_to_analysis}/{analysis_files[i]}/imls.csv', header= None)[1]
    elif im == 'SA(0.6s)':
        ims = pd.read_csv(f'{path_to_analysis}/{analysis_files[i]}/imls.csv', header= None)[2]
    elif im == 'SA(1.0s)':
        ims = pd.read_csv(f'{path_to_analysis}/{analysis_files[i]}/imls.csv', header= None)[3]
    elif im == 'AvgSa(0.3s)':
        ims = pd.read_csv(f'{path_to_analysis}/{analysis_files[i]}/avg_sas.csv', header= None)[1]
    elif im == 'AvgSa(0.6s)':
        ims = pd.read_csv(f'{path_to_analysis}/{analysis_files[i]}/avg_sas.csv', header= None)[2]
    elif im == 'AvgSa(1.0s)':
        ims = pd.read_csv(f'{path_to_analysis}/{analysis_files[i]}/avg_sas.csv', header= None)[3]
        
    # get the psd file
    psd = pd.read_csv(f'{path_to_analysis}/{analysis_files[i]}/max_interstorey_drift_ratio.csv', header= None).to_numpy()

    # get the pfa file
    pfa = pd.read_csv(f'{path_to_analysis}/{analysis_files[i]}/max_relative_floor_accel_g.csv', header= None).to_numpy()
    
    # get the number of stories
    nst = psd.shape[1]
    
    collLimit = 2.0
    
    # do vulnerability analysis
    total_loss, floor_loss_pfa, storey_loss_psd, x, y= vulnModel.processVulnerability(nst,slfs,ims,psd,pfa,outDir, collLimit, fun='binomial')
